import time
import ollama

# é…ç½®å‚æ•°
model = "deepseek-r1:32b"
prompt = "è¯·ä»‹ç»ä¸€ä¸‹ä¸­å›½çš„å†å²äººç‰©å±ˆåŸã€‚"
num_trials = 3  # è¿è¡Œæ¬¡æ•°

print("ğŸ”¥ é¢„çƒ­æ¨¡å‹ä¸­ï¼Œè¯·ç¨å€™...")
ollama.chat(model=model, messages=[{"role": "user", "content": "ä½ å¥½"}])  # é¢„çƒ­
print("âœ… é¢„çƒ­å®Œæˆï¼Œå¼€å§‹æ­£å¼æ¨ç†...\n")

# å­˜å‚¨ token é€Ÿç‡
speeds = []

for i in range(1, num_trials + 1):
    print(f"ğŸš€ è¿è¡Œç¬¬ {i} è½®æ¨ç†...")

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # è¿›è¡Œæ¨ç†ï¼Œ**ç¦ç”¨æµå¼è¾“å‡º**
    response = ollama.chat(model=model, messages=[{"role": "user", "content": prompt}], stream=False)

    # è®°å½•ç»“æŸæ—¶é—´
    end_time = time.time()
    elapsed_time = end_time - start_time

    # æå–ç»Ÿè®¡æ•°æ®
    eval_count = response.get("eval_count", 0)  # ç”Ÿæˆçš„ Token æ•°
    eval_duration = response.get("eval_duration", 1) / 1e9  # ç”Ÿæˆæ—¶é—´ï¼ˆç§’ï¼‰
    prompt_eval_count = response.get("prompt_eval_count", 0)  # æç¤ºè¯ Token æ•°
    prompt_eval_duration = response.get("prompt_eval_duration", 1) / 1e9  # æç¤ºè¯å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
    total_duration = response.get("total_duration", 1) / 1e9  # æ€»è€—æ—¶ï¼ˆç§’ï¼‰

    # è®¡ç®— Token é€Ÿç‡
    speed = eval_count / eval_duration if eval_duration > 0 else 0
    speeds.append(speed)

    print(f"ğŸ“œ æç¤ºè¯ Token æ•°: {prompt_eval_count}, å¤„ç†è€—æ—¶: {prompt_eval_duration:.2f} ç§’")
    print(f"ğŸ“œ ç”Ÿæˆ Token æ•°: {eval_count}, ç”Ÿæˆè€—æ—¶: {eval_duration:.2f} ç§’")
    print(f"â±ï¸  æ€»æ¨ç†æ—¶é—´: {total_duration:.2f} ç§’")
    print(f"âš¡ Token é€Ÿç‡: {speed:.2f} tokens/sec")
    print("ğŸ“¢ **æ¨¡å‹è¾“å‡º:**")
    print(response.get("message", {}).get("content", ""))  # è¾“å‡ºå®Œæ•´æ–‡æœ¬
    print("-" * 80 + "\n")

# è®¡ç®—å¹³å‡é€Ÿåº¦
avg_speed = sum(speeds) / len(speeds)
print(f"ğŸ† å¹³å‡ Token é€Ÿç‡: {avg_speed:.2f} tokens/sec")